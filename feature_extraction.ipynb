{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition with Inception CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning, in particular, deep learning is awesome in what it can accomplish. It is also computationally taxing and training a convolutional neural network on 1,200,000 images could take months on a laptop. Luckily, Google has not only developed their own Python library for deep learning, Tensorflow, and shared it with everyone, but they have also shared their own networks that have already been trained on the 1.2 million images across 1000 different classes in ImageNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "import os\n",
    "\n",
    "# Base url\n",
    "TF_MODELS_URL = \"http://download.tensorflow.org/models/\"\n",
    "\n",
    "# Modify this path for a different CNN\n",
    "INCEPTION_V3_URL = TF_MODELS_URL + \"inception_v3_2016_08_28.tar.gz\"\n",
    "INCEPTION_V4_URL = TF_MODELS_URL + \"inception_v4_2016_09_09.tar.gz\"\n",
    "\n",
    "# Directory to save model checkpoints\n",
    "MODELS_DIR = \"models/cnn\"\n",
    "\n",
    "INCEPTION_V3_CKPT_PATH = MODELS_DIR + \"/inception_v3.ckpt\"\n",
    "INCEPTION_V4_CKPT_PATH = MODELS_DIR + \"/inception_v4.ckpt\"\n",
    "\n",
    "# Make the model directory if it does not exist\n",
    "if not tf.gfile.Exists(MODELS_DIR):\n",
    "    tf.gfile.MakeDirs(MODELS_DIR)\n",
    " \n",
    "# Download the appropriate model if haven't already done so\n",
    "if not os.path.exists(INCEPTION_V3_CKPT_PATH):    \n",
    "    dataset_utils.download_and_uncompress_tarball(INCEPTION_V3_URL, MODELS_DIR)\n",
    "    \n",
    "if not os.path.exists(INCEPTION_V4_CKPT_PATH):\n",
    "    dataset_utils.download_and_uncompress_tarball(INCEPTION_V4_URL, MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the images into correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-46c5d4a9ce21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from preprocessing import inception_preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "#from preprocessing import inception_preprocessing\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "image_list = []\n",
    "\n",
    "for filename in glob.glob('/home/eaa418/Dataset/pics/*.jpg'):\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "for filename in glob.glob('/home/eaa418/Dataset/pics/*.png'):\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "    \n",
    "# with tf.Session() as sess:\n",
    "#     im = tf.convert_to_tensor(image_list, dtype=float32)\n",
    "    \n",
    "return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "# This can be modified depending on the model used and the training image dataset\n",
    "\n",
    "# def process_image(image):\n",
    "#     root_dir = \"/Users/eashanadhikarla/Desktop/Independent_Study/Dataset/pics/\"\n",
    "#     filename = root_dir + image\n",
    "    ########################################## Iterate it for all images. \n",
    "def process_image(image_list):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        image_str = f.read()\n",
    "        \n",
    "    if image.endswith('jpg'):\n",
    "        raw_image = tf.image.decode_jpeg(image_str, channels=3)\n",
    "    elif image.endswith('png'):\n",
    "        raw_image = tf.image.decode_png(image_str, channels=3)\n",
    "    else: \n",
    "        print(\"Image must be either jpg or png\")\n",
    "        return \n",
    "    \n",
    "    image_size = 299 # ImageNet image size, different models may be sized differently\n",
    "    processed_image = inception_preprocessing.preprocess_image(raw_image, image_size,\n",
    "                                                             image_size, is_training=False)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        raw_image, processed_image = sess.run([raw_image, processed_image])\n",
    "\n",
    "for filename in image_list:\n",
    "    process_image(image_list)\n",
    "    return raw_image, processed_image.reshape(-1, 299, 299, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image.astype(np.uint8), interpolation='nearest')\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several representative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dog, processed_dog = process_image('8a0cvc.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_color_image(raw_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scene, processed_scene = process_image('8a9rdn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_color_image(raw_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_dog.shape, processed_dog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_scene.shape, processed_scene.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it looks like the images are being properly formatted for use in the convolutional neural net. In the case of the bison image, because it is originally too small, the preprocessing function adds extra pixels by interpolating the colr value from surrounding pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pre-Trained Architecture and Model Weights and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import imagenet\n",
    "from tensorflow.contrib import slim\n",
    "from nets import inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predict(image, version) bFunction takes in the name of the image and optionally the network to use for predictions\n",
    "Currently, the only options for the net are Inception V3 and Inception V4.\n",
    "Plots the raw image and displays the top-10 class predictions.\n",
    "'''\n",
    "\n",
    "def predict(image, version='V3'):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Process the image \n",
    "    raw_image, processed_image = process_image(image)\n",
    "    class_names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "    \n",
    "    # Create a placeholder for the images\n",
    "    X = tf.placeholder(tf.float32, [None, 299, 299, 3], name=\"X\")\n",
    "    \n",
    "    '''\n",
    "    inception_v3 function returns logits and end_points dictionary\n",
    "    logits are output of the network before applying softmax activation\n",
    "    '''\n",
    "    \n",
    "    if version.upper() == 'V3':\n",
    "        model_ckpt_path = INCEPTION_V3_CKPT_PATH\n",
    "        with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "            # Set the number of classes and is_training parameter  \n",
    "            logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=False)\n",
    "            print (end_points.keys())\n",
    "            \n",
    "    elif version.upper() == 'V4':\n",
    "        model_ckpt_path = INCEPTION_V4_CKPT_PATH\n",
    "        with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "            # Set the number of classes and is_training parameter\n",
    "            # Logits \n",
    "            logits, end_points = inception.inception_v4(X, num_classes=1001, is_training=False)\n",
    "            print (end_points.keys())\n",
    "            \n",
    "    \n",
    "    predictions = end_points.get('Predictions', 'No key named predictions')\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_ckpt_path)\n",
    "        prediction_values = predictions.eval({X: processed_image})\n",
    "        \n",
    "        mix7c_output = sess.run(end_points['Mixed_7c'], feed_dict={X: processed_image})\n",
    "        print(mix7c_output)\n",
    "#         op = sess.graph.get_operations()\n",
    "#         [m.values() for m in op][1]\n",
    "        \n",
    "    try:\n",
    "        # Add an index to predictions and then sort by probability\n",
    "        prediction_values = [(i, prediction) for i, prediction in enumerate(prediction_values[0,:])]\n",
    "        prediction_values = sorted(prediction_values, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Plot the image\n",
    "        plot_color_image(raw_image)\n",
    "        plt.show()\n",
    "        print(\"Using Inception_{} CNN\\nPrediction: Probability\\n\".format(version))\n",
    "        # Display the image and predictions \n",
    "        for i in range(10):\n",
    "            predicted_class = class_names[prediction_values[i][0]]\n",
    "            probability = prediction_values[i][1]\n",
    "            print(\"{}: {:.2f}%\".format(predicted_class, probability*100))\n",
    "    \n",
    "    # If the predictions do not come out right\n",
    "    except:\n",
    "        print(predictions)\n",
    "    \n",
    "    return mix7c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inception Object Recoginition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix7c_output = predict('8a18m5.jpg', version='V3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict('8a18m5.jpg', version='V4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict('8a12dk.jpg', version='V4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle_out = open(\"/Users/eashanadhikarla/Desktop/Independent_Study/Output_Pickle/second_last_layer.pickle\",\"wb\")\n",
    "record = {}\n",
    "record['mix7c_output'] = mix7c_output\n",
    "\n",
    "\n",
    "pickle.dump(record, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"/Users/eashanadhikarla/Desktop/Independent_Study/Output_Pickle/second_last_layer.pickle\",\"rb\")\n",
    "dict_ = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "print(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mix7c_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
